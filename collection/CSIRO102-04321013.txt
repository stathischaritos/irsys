 The Hmisc Package
February 13, 2007
Version 3.2-1 Date 2007-12-25 Title Harrell Miscellaneous Author Frank E Harrell Jr , with contributions from many other users. Maintainer Charles Dupont Depends R (>= 2.4.0), methods Suggests lattice, grid, nnet, acepack, foreign, survival, chron, TeachingDemos, Design, cluster Description The Hmisc library contains many functions useful for data analysis, high-level graphics, utility operations, functions for License GPL version 2 or newer URL http://biostat.mc.vanderbilt.edu/s/Hmisc, http://biostat.mc.vanderbilt.edu/twiki/pub/Main/RS/sintro.pdf, http://biostat.mc.vanderbilt.edu/twiki/pub/Main/StatReport/summary.pdf, http://biostat.mc.vanderbilt.edu/trac/Hmisc R topics documented:
Cs . . . . . . . Ecdf . . . . . . Hmisc-internal Lag . . . . . . Misc . . . . . . Overview . . . Save . . . . . . abs.error.pred . all.is.numeric . approxExtrap . areg . . . . . . aregImpute . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 5 9 9 10 13 20 21 22 23 24 27 2 binconf . . . . . . . . . . . . bootkm . . . . . . . . . . . . bpower . . . . . . . . . . . . bpplot . . . . . . . . . . . . . bystats . . . . . . . . . . . . . ceiling.chron . . . . . . . . . ciapower . . . . . . . . . . . . contents . . . . . . . . . . . . cpower . . . . . . . . . . . . . csv.get . . . . . . . . . . . . . curveRep . . . . . . . . . . . cut2 . . . . . . . . . . . . . . data.frame.create.modify.check dataRep . . . . . . . . . . . . deff . . . . . . . . . . . . . . describe . . . . . . . . . . . . dotchart2 . . . . . . . . . . . dropUnusedLevels . . . . . . eip . . . . . . . . . . . . . . . equalBins . . . . . . . . . . . errbar . . . . . . . . . . . . . escapeRegex . . . . . . . . . . event.chart . . . . . . . . . . . event.history . . . . . . . . . . ﬁnd.matches . . . . . . . . . . ﬁrst.word . . . . . . . . . . . format.df . . . . . . . . . . . format.pval . . . . . . . . . . gbayes . . . . . . . . . . . . . getHdata . . . . . . . . . . . . getZip . . . . . . . . . . . . . hdquantile . . . . . . . . . . . hist.data.frame . . . . . . . . . histbackback . . . . . . . . . hoeffd . . . . . . . . . . . . . html . . . . . . . . . . . . . . impute . . . . . . . . . . . . . labcurve . . . . . . . . . . . . label . . . . . . . . . . . . . . latex . . . . . . . . . . . . . . ldBands . . . . . . . . . . . . list.tree . . . . . . . . . . . . mApply . . . . . . . . . . . . makeNstr . . . . . . . . . . . mgp.axis . . . . . . . . . . . . minor.tick . . . . . . . . . . . mtitle . . . . . . . . . . . . . na.delete . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . R topics documented: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 36 37 40 41 44 44 46 48 50 52 57 58 66 69 70 74 76 77 77 78 80 81 90 96 100 101 104 105 111 113 114 115 116 118 119 121 123 132 135 143 147 148 150 150 152 153 154 R topics documented: na.detail.response . na.keep . . . . . . %nin% . . . . . . . panel.bpplot . . . . partition . . . . . . pc1 . . . . . . . . . plotCorrPrecision . plsmo . . . . . . . popower . . . . . . print.char.list . . . print.char.matrix . . prnz . . . . . . . . ps.slide . . . . . . pstamp . . . . . . . rMultinom . . . . . rcorr . . . . . . . . rcorr.cens . . . . . rcorrp.cens . . . . rcspline.eval . . . . rcspline.plot . . . . rcspline.restate . . reShape . . . . . . reorder.factor . . . requirePackage . . rlegend . . . . . . rm.boot . . . . . . samplesize.bin . . . sasxport.get . . . . scat1d . . . . . . . score.binary . . . . sedit . . . . . . . . show.pch . . . . . simplifyDims . . . smean.sd . . . . . . solvet . . . . . . . somers2 . . . . . . spower . . . . . . . spss.get . . . . . . src . . . . . . . . . stata.get . . . . . . store . . . . . . . . string.bounding.box string.break.line . . stringDims . . . . . summarize . . . . . summary.formula . symbol.freq . . . . sys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 155 156 157 158 161 162 163 163 167 168 170 171 172 177 178 179 181 183 184 185 187 188 192 193 193 195 202 204 207 213 214 217 218 219 220 221 222 227 228 229 230 232 233 234 235 239 254 256 4 t.test.cluster transace . . transcan . . translate . . units . . . . upData . . . valueTags . varclus . . . wtd.stats . . xYplot . . . xy.group . . sas.get . . . Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Cs 256 258 266 280 281 282 285 287 292 295 304 305 312 Cs Character strings from unquoted names Description Makes a vector of character strings from a list of valid S names Usage Cs(...) Arguments ... Value character string vector See Also sys.frame, deparse Examples
Cs(a,cat,dog) # subset.data.frame x or "f" to plot the cumulative frequency of values x", or "Frequency =1 factors 2-way statistics Calling tree of functions (David Lubinsky, david@hoqax.att.com ) Shows numeric equivalents of all latin characters Useful for putting many special chars. in graph titles (Pierre Joyet, pierre.joyet@bluewin.ch ) Power of Cox interaction test More compactly store variables in a data frame, and clean up problem data when e.g. Excel spreadsheet had a nonnumeric value in a numeric column Combine infrequent levels of a categorical variable Attach a comment attribute to an object: comment(ﬁt) 0 and "l" if nk=0. same coding as for xtype. Default is "s" for a numeric variable with more than two unique values, "l" for a binary numeric variable, and "c" for a factor, categorical, or character variable. ytype number of knots, 0 for linear, or 3 or more. Default is 4 which will ﬁt 3 parameters to continuous variables (one linear term and two nonlinear terms) linear.predictors set to TRUE to store predicted transformed y in the result nk B na.rm tolerance object whichx ... Details areg is a competitor of ace in the acepack package. Transformations from ace are seldom smooth enough and are often overﬁtted. With areg the complexity can be controlled with the nk parameter, and predicted values are easy to obtain because parametric functions are ﬁtted. If one side of the equation has a categorical variable with more than two categories and the other side has a continuous variable not assumed to act linearly, larger sample sizes are needed to reliably estimate transformations, as it is difﬁcult to optimally score categorical variables to maximize R2 against a simultaneously optimally transformed continuous variable. Value a list of class "areg" containing many objects Author(s) Frank Harrell Department of Biostatistics Vanderbilt University f.harrell@vanderbilt.edu References Breiman and Friedman, Journal of the American Statistical Association (September, 1985). number of bootstrap resamples used to estimate covariance matrices of transformation parameters. Default is no bootstrapping. set to FALSE if you are sure that observations with NAs have already been removed singularity tolerance. List source code for lm.fit.qr.bare for details. an object created by areg integer or character vector specifying which predictors are to have their transformations plotted (default is all). The y transformation is always plotted. arguments passed to the plot function. 26 See Also cancor,ace, transcan Examples
set.seed(1) areg ns 0, initialize the NAs to values from a random sample (without replacement if a sufﬁcient number of non-missing values exist) of size m from the non-missing values. (2) For burnin+n.impute iterations do the following steps. The ﬁrst burnin iterations provide a burn-in, and imputations are saved only from the last n.impute iterations. (3) For each variable containing any NAs, draw a sample with replacement from the observations in the entire dataset in which the current variable being imputed is non-missing. Fit a ﬂexible additive model to predict this target variable while ﬁnding the optimum transformation of it (unless the identity transformation is forced). Use this ﬁtted ﬂexible model to predict the target variable in all of the original observations. Impute each missing value of the target variable with the observed value whose predicted transformed value is closest to the predicted transformed value of the missing value (if match="closest" and type="pmm"), or use a draw from a multinomial distribution with probabilities derived from distance weights, if match="weighted" (the default). (4) After these imputations are computed, use these random draw imputations the next time the curent target variable is used as a predictor of other sometimes-missing variables. When match="closest", predictive mean matching does not work well when fewer than 3 variables are used to predict the target variable, because many of the multiple imputations for an observation will be identical. In the extreme case of one right-hand-side variable and assuming that only monotonic transformations of left and right-side variables are allowed, every bootstrap resample will give predicted values of the target variable that are monotonically related to predicted values from every other bootstrap resample. The same is true for Bayesian predicted values. This causes predictive mean matching to always match on the same donor observation. When the missingness mechanism for a variable is so systematic that the distribution of observed values is truncated, predictive mean matching does not work. It will only yield imputed values that are near observed values, so intervals in which no values are observed will not be populated by imputed values. For this case, the only hope is to make regression assumptions and use extrapolation. With type="regression", aregImpute will use linear extrapolation to obtain a (hopefully) reasonable distribution of imputed values. The "regression" option causes aregImpute to impute missing values by adding a random sample of residuals (with replacement if there are more NAs than measured values) on the transformed scale of the target variable. After random residuals are added, predicted random draws are obtained on the original untransformed scale using reverse linear interpolation on the table of original and transformed target values (linear extrapolation when a random residual is large enough to put the random draw prediction outside the range of observed values). The bootstrap is used as with type="pmm" to factor in the uncertainty of the imputation model. Value a list of class "aregImpute" containing the following elements: call the function call expression aregImpute formula match fweighted n p na nna type nk cat.levels df n.impute imputed the formula speciﬁed to aregImpute the match argument the fweighted argument total number of observations in input dataset number of variables list of subscripts of observations for which values were originally missing named vector containing the numbers of missing values in the data 31 vector of types of transformations used for each variable ("s","l","c" for smooth spline, linear, or categorical with dummy variables) number of knots used for smooth transformations list containing character vectors specifying the levels of categorical variables degrees of freedom (number of parameters estimated) for each variable number of multiple imputations per missing value a list containing matrices of imputed values in the same format as those created by transcan. Categorical variables are coded using their integer codes. Variables having no missing values will have NULL matrices in the list. if x is TRUE, the original data matrix with integer codes for categorical variables for the last round of imputations, a vector containing the R-squares with which each sometimes-missing variable could be predicted from the others by ace or avas. x rsq Author(s) Frank Harrell Department of Biostatistics Vanderbilt University f.harrell@vanderbilt.edu References Little R, An H. Robust likelihood-based analysis of multivariate data with missing values. Statistica Sinica 14:933-952, 2004. van Buuren S, Brand JPL, Groothuis-Oudshoorn CGM, Rubin DB. Fully conditional speciﬁcations in multivariate imputation. Draft available from http://web.inter.nl.net/users/S. van.Buuren/publications/FCS%20(revised%20Jan%202005).pdf. See Also fit.mult.impute, transcan, areg, naclus, naplot, mice, dotchart2, Ecdf 32 Examples aregImpute # Check that aregImpute can almost exactly estimate missing values when # there is a perfect nonlinear relationship between two variables # Fit restricted cubic splines with 4 knots for x1 and x2, linear for x3 set.seed(3) x1 =0) nc.i max(cuts), augments cuts to include min and max x if an interval contains only one unique value, the interval will be labeled with the formatted version of that value instead of the interval endpoints, unless oneval=FALSE set to TRUE to only return the vector of computed cuts. This consists of the interior values plus outer ranges. onlycuts Value a factor variable with levels of the form [a,b) or formatted means (character strings) unless onlycuts is TRUE in which case a numeric vector is returned See Also cut, quantile Examples
set.seed(1) x 2 values by(FEV, FEV$smoke, summary) # use basic summary function with stratification # ----------------------------------------------------------------------- 64
# # # # # # # # # Step 7: data.frame.create.modify.check
Do detailed analyses involving individual variables Analyses based on the formula language can use data= so attaching the data frame may not be required. This saves memory. Here we use the Hmisc summary.formula function to compute 5 statistics on height, stratified separately by age quartile and by sex. options(width=80) summary(height ~ age + sex, data=FEV, fun=function(y)c(smean.sd(y), smedian.hilow(y,conf.int=.5))) # This computes mean height, S.D., median, outer quartiles fit in order for the event chart to be displayed. This feature is possible due to the dev.ask option. If ’i’, an internal legend will be placed in the plot region based on legend.point.at. If ’l’, a legend will be placed in the plot region using the locator option. Legend will map points to events (via column names, by default) and, if line.by is speciﬁed, lines to groups (based on levels of line.by). legend.titl title for the legend; default is title to be used for main plot. Only used when legend.location = ’o’. legend.titl.cex size of text for legend title. Only used when legend.location = ’o’. legend.titl.line line location of legend title dictated by mtext function with outer=FALSE option; default is 1.0. Only used when legend.location = ’o’. legend.point.at location of upper left and lower right corners of legend area to be utilized for describing events via points and text. event.chart 87 legend.point.pch vector of pch values for points representing each event in the legend. Default is point.pch. legend.point.text text to be used for describing events; the default is setup for a data frame, as it will print the names of the columns speciﬁed by subset.c . legend.cex size of text for points and event descriptions. Default is 2.5 which is setup for legend.location = ’o’. A much smaller cex is recommended (possibly 0.75) for use with legend.location = ’i’ or legend.location = ’l’. option to put a box around the legend(s); default is to have no box (legend.bty = ’n’). Option legend.bty = ’o’ will produce a legend box. legend.line.at if line.by was speciﬁed (with legend.location = ’o’ or legend.location = ’i’), this argument will dictate the location of the upper left and lower right corners of legend area to be utilized for describing the different line.by values (e.g., treatment.arm). The default is setup for legend.location == ’o’. legend.line.text text to be used for describing line.by values; the default are the names of the unique non-missing line.by values as produced from the table function. legend.line.lwd vector of line widths corresponding to line.by values. legend.loc.num number used for locator argument when legend.locator = ’l’. If 1 (default), user is to locate only the top left corner of the legend box. If 2, user is to locate both the top left corner and the lower right corner. This will be done twice when line.by is speciﬁed (once for points and once for lines). legend.bty event.time event.code ... Details if you want to put, say, two eventcharts side-by-side, in a plot region, you should not set up par(mfrow=c(1,2)) before running the ﬁrst plot. Instead, you should add the argument mfg=c(1,1,1,2) to the ﬁrst plot call followed by the argument mfg=c(1,2,1,2) to the second plot call. if dates in original data frame are in a specialized form (eg., mm/dd/yy) of mode CHARACTER, the user must convert those columns to become class dates or julian numeric mode (see ?dates for more information). For example, in a data frame called testdata, with specialized dates in columns 4 thru 10, the following code could be used: as.numeric(dates(testdata[,4:10]). This will convert the columns to numeric julian dates based on the function’s default origin of January 1, 1960. If original dates are in class dates or julian form, no extra work is necessary. In the survival analysis, the data typically come in two columns: one column containing survival time and the other containing censoring indicator or event code. The event.convert function converts this type of data into multiple columns of event times, one column of each event type, suitable for the event.chart function. the column number in data contains the event time the column number in data contains the event code additional par arguments for use in main plot. 88 Side Effects event.chart an event chart is created on the current graphics device. If legend.plot =TRUE and legend.location = ’o’, a one-page legend will precede the event chart. Please note that par parameters on completion of function will be reset to par parameters existing prior to start of function. Author(s) J. Jack Lee and Kenneth R. Hess Department of Biostatistics University of Texas M.D. Anderson Cancer Center Houston, TX 77030 jjlee@mdanderson.org, khess@mdanderson.org Joel A. Dubin Division of Biostatistics Department of Epidemiology and Public Health Yale University joel.dubin@yale.edu References Lee J.J., Hess, K.R., Dubin, J.A. (2000). Extensions and applications of event charts. The American Statistician, 54:1, 63–70. Dubin, J.A., Lee, J.J., Hess, K.R. (1997). The Utility of Event Charts. Proceedings of the Biometrics Section, American Statistical Association. Dubin, J.A., Muller H-G, Wang J-L (2001). Event history graphs for censored survival data. Statistics in Medicine, 20: 2951–2964. Goldman, A.I. (1992). EVENTCHARTS: Visualizing Survival and Other Timed-Events Data. The American Statistician, 46:1, 13–18. See Also event.history Examples
# # # # # # # # # # # The sample data set is an augmented CDC AIDS dataset (ASCII) which is used in the examples in the help file. This dataset is described in Kalbfleisch and Lawless (JASA, 1989). Here, we have included only children 4 years old and younger. We have also added a new field, dethdate, which represents a fictitious death date for each patient. There was no recording of death date on the original dataset. All dates are julian with julian=0 being January 1, 1960, and julian=14000 being 14000 days beyond January 1, 1960 (i.e., May 1, 1998). cdcaids = 0), survtime.col=heart.one[,2], surv.col=heart.one[,3], covtime.cols = cbind(rep(0, dim(heart.one)[1]), heart.one[,1]), cov.cols = cbind(rep(0, dim(heart.one)[1]), heart.one[,7]), num.colors=2, colors=c(6,10), x.lab = 'time under observation\n(in days)', title = 'Figure 5b:\nStanford data\n(age >= 48)', cens.mark.right =TRUE, cens.mark = '-', cens.mark.ahead = 40.0, cens.mark.cex = 0.85, xlim=c(0,1900)) # dev.off() # par(omi=omi) # we will not show liver cirrhosis data manipulation, as it was # a bit detailed; however, here is the # event.history code to produce Figure 7 / Plate 1 # Figure 7 / Plate 1 : prothrombin ehg with color ## Not run: second.arg 0)) j 2])[1] x[j,] y[w$matches[j,],] summary(w) # # # # # # For many applications would do something like this: attach(df1) x 0 & n.control > 0 denom delta.w for the case where (1) a Gaussian prior or mixture of two Gaussian priors is used as the prior distribution, (2) this prior is used in forming the statistical test or credible interval, (3) no prior is used for the distribution of delta for computing power but instead a ﬁxed single delta is given (as in traditional frequentist hypothesis tests), and (4) the test statistic has a Gaussian likelihood with known variance (and mean equal to the speciﬁed delta). gbayesMixPowerNP is handy where you want to use an earlier study in testing for treatment effects in a new study, but you want to mix with this prior a non-informative prior. The mixing probability mix can be thought of as the "applicability" of the previous study. As with gbayes2, power here means the probability that the new study will yield 106 gbayes a left credible interval that is to the right of delta.w. gbayes1PowerNP is a special case of gbayesMixPowerNP when the prior is a single Gaussian. Usage gbayes(mean.prior, var.prior, m1, m2, stat, var.stat, n1, n2, cut.prior, cut.prob.prior=0.025) ## S3 method for class 'gbayes': plot(x, xlim, ylim, name.stat='z', ...) gbayes2(sd, prior, delta.w=0, alpha=0.05, upper=Inf, prior.aux) gbayesMixPredNoData(mix=NA, d0=NA, v0=NA, d1=NA, v1=NA, what=c('density','cdf')) gbayesMixPost(x=NA, v=NA, mix=1, d0=NA, v0=NA, d1=NA, v1=NA, what=c('density','cdf')) gbayesMixPowerNP(pcdf, delta, v, delta.w=0, mix, interval, nsim=0, alpha=0.05) gbayes1PowerNP(d0, v0, delta, v, delta.w=0, alpha=0.05) Arguments mean.prior mean of the prior distribution cut.prior cut.prob.prior var.prior variance of the prior. Use a large number such as 10000 to effectively use a ﬂat (noninformative) prior. Sometimes it is useful to compute the variance so that the prior probability that stat is greater than some impressive value u is only alpha. The correct var.prior to use is then ((u-mean.prior)/qnorm(1alpha))^2. You can specify cut.prior=u and cut.prob.prior=alpha (whose default is 0.025) in place of var.prior to have gbayes compute the prior variance in this manner. sample size in group 1 sample size in group 2 statistic comparing groups 1 and 2, e.g., log hazard ratio, difference in means, difference in angular transformations of proportions variance of stat, assumed to be known. var.stat should either be a constant (allowed if n1 is not speciﬁed), or a function of two arguments which specify the sample sizes in groups 1 and 2. Calculations will be approximate when the variance is estimated from the data. an object returned by gbayes or the value of the statistic which is an estimator of delta, the parameter of interest m1 m2 stat var.stat x gbayes sd prior pcdf delta v the standard deviation of the treatment effect 107 a function of possibly a vector of unknown treatment effects, returning the prior density at those values a function computing the posterior CDF of the treatment effect delta, such as a function created by gbayesMixPost with what="cdf". a true unknown single treatment effect to detect the variance of the statistic x, e.g., s^2 * (1/n1 + 1/n2). Neither x nor v need to be deﬁned to gbayesMixPost, as they can be deﬁned at run time to the function created by gbayesMixPost. number of future observations in group 1, for obtaining a predictive distribution number of future observations in group 2 vector of 2 x-axis limits. Default is the mean of the posterior plus or minus 6 standard deviations of the posterior. vector of 2 y-axis limits. Default is the range over combined prior and posterior densities. label for x-axis. Default is "z". optional arguments passed to labcurve from plot.gbayes the minimum worthwhile treatment difference to detech. The default is zero for a plain uninteristing null hypothesis. type I error, or more accurately one minus the conﬁdence level for a two-sided conﬁdence limit for the treatment effect upper limit of integration over the prior distribution multiplied by the normal likelihood for the treatment effect statistic. Default is inﬁnity. argument to pass to prior from integrate through gbayes2. Inside of power the argument must be named prior.aux if it exists. You can pass multiple parameters by passing prior.aux as a list and pulling off elements of the list inside prior. This setup was used because of difﬁculties in passing ... arguments through integrate for some situations. mixing probability or weight for the Gaussian prior having mean d0 and variance v0. mix must be between 0 and 1, inclusive. mean of the ﬁrst Gaussian distribution (only Gaussian for gbayes1PowerNP and is a required argument) variance of the ﬁrst Gaussian (only Gaussian for gbayes1PowerNP and is a required argument) mean of the second Gaussian (if mix delta.w, and the second value is the power. If nsim > 0, it returns the power estimate and conﬁdence limits for it if nsim > 0. The examples show how to use these functions. Author(s) Frank Harrell Department of Biostatistics Vanderbilt University School of Medicine f.harrell@vanderbilt.edu References Spiegelhalter DJ, Freedman LS, Parmar MKB (1994): Bayesian approaches to randomized trials. JRSS A 157:357–416. Results for gbayes are derived from Equations 1, 2, 3, and 6. Spiegelhalter DJ, Freedman LS (1986): A predictive approach to selecting the size of a clinical trial, based on subjective clinical opinion. Stat in Med 5:1–13. Joseph, Lawrence and Belisle, Patrick (1997): Bayesian sample size determination for normal means and differences between normal means. The Statistician 46:209–226. Examples
# Compare 2 proportions using the var stabilizing transformation # arcsin(sqrt((x+3/8)/(n+3/4))) (Anscombe), which has variance # 1/[4(n+.5)] m1 w] use # 1-pnorm(w, b$mean.post, sqrt(b$var.post)) #If g(effect, n1, n2) is the power function to #detect an effect of 'effect' with samples size for groups 1 and 2 #of n1,n2, estimate the expected power by getting 1000 random #draws from the posterior distribution, computing power for #each value of the population effect, and averaging the 1000 powers #This code assumes that g will accept vector-valued 'effect' #For the 2-sample proportion problem just addressed, 'effect' #could be taken approximately as the change in the arcsin of #the square root of the probability of the event g 0.5, P values are computed using a wellﬁtting linear regression function in log P vs. the test statistic. Ranks (but not bivariate ranks) are computed using efﬁcient algorithms (see reference 3). Value a list with elements D, the matrix of D statistics, n the matrix of number of observations used in analyzing each pair of variables, and P, the asymptotic P-values. Pairs with fewer than 5 nonmissing values have the D statistic set to NA. The diagonals of n are the number of non-NAs for the single variable corresponding to that row and column. a numeric matrix with at least 5 rows and at least 2 columns (if y is absent), or an object created by hoeffd a numeric vector or matrix which will be concatenated to x ignored html Author(s) Frank Harrell Department of Biostatistics Vanderbilt University f.harrell@vanderbilt.edu References Hoeffding W. (1948): A non-parametric test of independence. Ann Math Stat 19:546–57. 119 Hollander M. and Wolfe D.A. (1973). Nonparametric Statistical Methods, pp. 228–235, 423. New York: Wiley. Press WH, Flannery BP, Teukolsky SA, Vetterling, WT (1988): Numerical Recipes in C. Cambridge: Cambridge University Press. See Also rcorr, varclus Examples
x 5, d=101:103) sapply(w, function(x){ hist(as.numeric(x), xlab=label(x)) # locator(1) ## wait for mouse click }) # Or: for(u in w) {hist(u); title(label(u))} 135 latex Convert an S object to LaTeX, and Related Utilities Description latex converts its argument to a .tex ﬁle appropriate for inclusion in a LaTeX2e document. latex is a generic function that calls one of latex.default, latex.function, latex.list. latex.default does appropriate rounding and decimal alignment and produces a ﬁle containing a LaTeX tabular environment to print the matrix or data.frame x as a table. latex.function prepares an S function for printing by issuing sed commands that are similar to those in the S.to.latex procedure in the s.to.latex package (Chambers and Hastie, 1993). latex.list calls latex recursively for each element in the argument. latexTranslate translates particular items in character strings to LaTeX format, e.g., makes a^2 = a$^2$ for superscript within variable labels. LaTeX names of greek letters (e.g., "alpha") will have backslashes added if greek==TRUE. Math mode is inserted as needed. latexTranslate assumes that input text always has matches, e.g. [) [] (] (), and that surrounding by $$ is OK. latexSN converts a vector ﬂoating point numbers to character strings using LaTeX exponents. Dollar signs to enter math mode are not added. latexVerbatim on an object executes the object’s print method, capturing the output for a ﬁle inside a LaTeX verbatim environment. dvi uses the system latex command to compile LaTeX code produced by latex, including any needed styles. dvi will put a documentclass{report} and end{document} wrapper around a ﬁle produced by latex. By default, the geometry LaTeX package is used to omit all margins and to set the paper size to a default of 5.5in wide by 7in tall. The result of dvi is a .dvi ﬁle. To both format and screen display a non-default size, use for example print(dvi(latex(x), width=3, height=4),width=3,height=4). Note that you can use something like xdvi -geometry 460x650 -margins 2.25in file without changing LaTeX defaults to emulate this. dvips will use the system dvips command to print the .dvi ﬁle to the default system printer, or create a postscript ﬁle if file is speciﬁed. 136 latex dvigv uses the system dvips command to convert the input object to a .dvi ﬁle, and uses the system dvips command to convert it to postscript. Then the postscript ﬁle is displayed using Ghostview (assumed to be the system command gv). There are show methods for displaying typeset LaTeX on the screen using the system xdvi command. If you show a LaTeX ﬁle created by latex without running it through dvi using show.dvi(object), the show method will run it through dvi automatically. These show methods are not S Version 4 methods so you have to use full names such as show.dvi and show.latex. Use the print methods for more automatic display of typesetting, e.g. typing latex(x) will invoke xdvi to view the typeset document. Usage latex(object, title=first.word(deparse(substitute(object))), ...) ## Default S3 method: latex(object, title=first.word(deparse(substitute(object))), file=paste(title, ".tex", sep=""), append=FALSE, label=title, rowlabel=title, rowlabel.just="l", cgroup=NULL, n.cgroup=NULL, rgroup=NULL, n.rgroup=NULL, cgroupTexCmd="bfseries", rgroupTexCmd="bfseries", rownamesTexCmd=NULL, colnamesTexCmd=NULL, cellTexCmds=NULL, rowname, cgroup.just=rep("c",length(n.cgroup)), colheads=dimnames(cx)[[2]], extracolheads=NULL, extracolsize='scriptsize', dcolumn=FALSE, numeric.dollar=!dcolumn, cdot=FALSE, longtable=FALSE, draft.longtable=TRUE, ctable=FALSE, booktabs=FALSE, table.env=TRUE, here=FALSE, lines.page=40, caption=NULL, caption.lot=NULL, caption.loc=c('top','bottom'), double.slash=FALSE, vbar=FALSE, collabel.just=rep("c",nc), na.blank=TRUE, insert.bottom=NULL, first.hline.double=!(booktabs ctable), where='!tbp', size=NULL, center=c('center','centering','none'), landscape=FALSE, multicol=TRUE, math.row.names=FALSE, math.col.names=FALSE, ...) # x is a matrix or data.frame ## S3 method for class 'function': latex( object, title=first.word(deparse(substitute(object))), file=paste(title, ".tex", sep=""), latex append=FALSE, assignment=TRUE, 137 type=c('example','verbatim'), ...) ## S3 method for class 'list': latex( object, title=first.word(deparse(substitute(object))), file=paste(title, ".tex", sep=""), append=FALSE, label, caption, caption.lot, caption.loc=c('top','bottom'), ...) ## S3 method for class 'latex': print(x, ...) latexTranslate(object, inn=NULL, out=NULL, pb=FALSE, greek=FALSE, ...) latexSN(x) latexVerbatim(x, title=first.word(deparse(substitute(x))), file=paste(title, ".tex", sep=""), append=FALSE, size=NULL, hspace=NULL, width=.Options$width, length=.Options$length, ...) dvi(object, ...) ## S3 method for class 'latex': dvi(object, prlog=FALSE, nomargins=TRUE, width=5.5, height=7, ...) ## S3 method for class 'dvi': print(x, ...) dvips(object, ...) ## S3 method for class 'latex': dvips(object, ...) ## S3 method for class 'dvi': dvips(object, file, ...) ## S3 method for class 'latex': show(object) # or show.dvi(object) or just object dvigv(object, ...) ## S3 method for class 'latex': dvigv(object, ...) # or gvdvi(dvi(object)) ## S3 method for class 'dvi': dvigv(object, ...) Arguments object For latex, any S object. For dvi or dvigv, an object created by latex. For latexTranslate is a vector of character strings to translate. 138 x title file latex any object to be printed verbatim for latexVerbatim. For latexSN x is a numeric vector. name of ﬁle to create without the .tex extension. name of the ﬁle to create. The default ﬁle name is x.tex where x is the ﬁrst word in the name of the argument for x. Set file="" to have the generated LaTeX code just printed to standard output. This is especially useful when running under Sweave in R using its results=tex tag, to save having to manage many small external ﬁles. When file="", latex keeps track of LaTeX styles that are called for by creating or modifying an object latexStyles (in .GlobalTemp in R or in frame 0 in S-Plus). latexStyles is a vector containing the base names of all the unique LaTeX styles called for so far in the current session. See the end of the examples section for a way to use this object to good effect. For dvips, file is the name of an output postscript ﬁle. defaults to FALSE. Set to TRUE to append output to an existing ﬁle. a text string representing a symbolic label for the table for referencing in the LaTeX \label and \ref commands. label is only used if caption is given. append label If x has row dimnames, rowlabel is a character string containing the column heading for the row dimnames. The default is the name of the argument for x. rowlabel.just If x has row dimnames, speciﬁes the justiﬁcation for printing them. Possible values are "l", "r", "c". The heading (rowlabel) itself is left justiﬁed if rowlabel.just="l", otherwise it is centered. rowlabel cgroup n.cgroup a vector of character strings deﬁning major column headings. The default is to have none. a vector containing the number of columns for which each element in cgroup is a heading. For example, specify cgroup=c("Major 1","Major 2"), n.cgroup=c(3,3) if "Major 1" is to span columns 1-3 and "Major 2" is to span columns 4-6. rowlabel does not count in the column numbers. You can omit n.cgroup if all groups have the same number of columns. a vector of character strings containing headings for row groups. n.rgroup must be present when rgroup is given. The ﬁrst n.rgroup[1] rows are sectioned off and rgroup[1] is used as a bold heading for them. The usual row dimnames (which must be present if rgroup is) are indented. The next n.rgroup[2] rows are treated likewise, etc. integer vector giving the number of rows in each grouping. If rgroup is not speciﬁed, n.rgroup is just used to divide off blocks of rows by horizontal lines. If rgroup is given but n.rgroup is omitted, n.rgroup will default so that each row group contains the same number of rows. rgroup n.rgroup cgroupTexCmd A character string specifying a LaTeX command to be used to format column group labels. The default, "bfseries", sets the current font to ’bold’. It is possible to supply a vector of strings so that each column group label is formatted differently. Please note that the ﬁrst item of the vector is used to format the title (even if a title is not used), and that there is an empty column between each column group. Currently the user needs to handle these issues. Multiple effects can be latex 139 achieved by creating custom LaTeX commands; for example, providecommand{ redscshape}{ color{red} scshape} creates a LaTeX command called redscshape that formats the text in red small-caps. rgroupTexCmd A character string specifying a LaTeX command to be used to format row group labels. The default, "bfseries", sets the current font to ’bold’. A vector of strings can be supplied to format each row group label differently. Normal recycling applies if the vector is shorter than n.rgroups. See also cgroupTexCmd above regarding multiple effects. rownamesTexCmd A character string specifying a LaTeX command to be used to format rownames. The default, NULL, applies no command. A vector of different commands can also be supplied. See also cgroupTexCmd above regarding multiple effects. colnamesTexCmd A character string specifying a LaTeX command to be used to format column labels. The default, NULL, applies no command. It is possible to supply a vector of strings to format each column label differently. If column groups are not used, the ﬁrst item in the vector will be used to format the title. Please note that if column groups are used the ﬁrst item of cgroupTexCmd and not colnamesTexCmd is used to format the title, and that there are empty columns between each group. The user needs to allow for these issues when supplying a vector of commands. See also cgroupTexCmd above regarding multiple effects. cellTexCmds A matrix of character strings which are LaTeX commands to be used to format each element, or cell, of the object. The matrix must have the same NROW() and NCOL() as the object. The default, NULL, applies no formats. Empty strings also apply no formats, and one way to start might be to create a matrix of empty strings with matrix(rep("", NROW(x) * NCOL(x)), nrow=NROW(x)) and then selectively change appropriate elements of the matrix. Note that you might need to set numeric.dollar=FALSE (to disable math mode) for some effects to work. See also cgroupTexCmd above regarding multiple effects. Set to TRUE to use blanks rather than NA for missing values. This usually looks better in latex. insert.bottom an optional character string to typeset at the bottom of the table. For "ctable" style tables, this is placed in an unmarked footnote. first.hline.double set to FALSE to use single horizontal rules for styles other than "bookmark" or "ctable" na.blank rowname cgroup.just colheads rownames for tabular environment. Default is rownames of matrix or data.frame. Specify rowname=NULL to suppress the use of row names. justiﬁcation for labels for column groups. Defaults to "c". a character vector of column headings if you don’t want to use dimnames(object)[[2]]. Specify colheads=NULL to suppress column headings. 140 latex extracolheads an optional vector of extra column headings that will appear under the main headings (e.g., sample sizes). This character vector does not need to include an empty space for any rowname in effect, as this will be added automatically. You can also form subheadings by splitting character strings deﬁning the column headings using the usual backslash n newline character. extracolsize size for extracolheads or for any second lines in column names; default is "scriptsize" dcolumn numeric.dollar logical, default !dcolumn. Set to TRUE to place dollar signs around numeric values when dcolumn=FALSE. This assures that latex will use minus signs rather than hyphens to indicate negative numbers. Set to FALSE when dcolumn=TRUE, as dcolumn.sty automatically uses minus signs. math.row.names logical, set true to place dollar signs around the row names. math.col.names logical, set true to place dollar signs around the column names. cdot longtable see format.df Set to TRUE to use David Carlisle’s LaTeX longtable style, allowing long tables to be split over multiple pages with headers repeated on each page. The "style" element is set to "longtable". The latex \usepackage must reference [longtable]. The ﬁle longtable.sty will need to be in a directory in your $TEXINPUTS path. draft.longtable I forgot what this does. ctable set to TRUE to use Wybo Dekker’s ctable style from CTAN. Even though for historical reasons it is not the default, it is generally the preferred method. Thicker but not doubled hlines are used to start a table when ctable is in effect. set booktabs=TRUE to use the booktabs style of horizontal rules for better tables. In this case, double hlines are not used to start a table. Set table.env=FALSE to suppress enclosing the table in a LaTeX table environment. table.env only applies when longtable=FALSE. You may not specify a caption if table.env=FALSE. Set to TRUE if you are using table.env=TRUE with longtable=FALSE and you have installed David Carlisle’s here.sty LaTeX style. This will cause the LaTeX table environment to be set up with option H to guarantee that the table will appear exactly where you think it will in the text. The "style" element is set to "here". The latex \usepackage must reference [here]. The ﬁle here.sty will need to be in a directory in your $TEXINPUTS path. here is largely obsolete with LaTeX2e. Applies if longtable=TRUE. No more than lines.page lines in the body of a table will be placed on a single page. Page breaks will only occur at rgroup boundaries. booktabs table.env here lines.page latex caption caption.lot 141 a text string to use as a caption to print at the top of the ﬁrst page of the table. Default is no caption. a text string representing a short caption to be used in the "List of Tables". By default, LaTeX will use caption. If you get inexplicable latex errors, you may need to supply caption.lot to make the errors go away. set to "bottom" to position a caption below the table instead of the default of "top". caption.loc double.slash set to TRUE to output \ as \\ in LaTeX commands. Useful when you are reading the output ﬁle back into an S vector for later output. logical. When vbar==TRUE, columns in the tabular environment are separated with vertical bar characters. When vbar==FALSE, columns are separated with white space. The default, vbar==FALSE, produces tables consistent with the style sheet for the Journal of the American Statistical Association. collabel.just justiﬁcation for column labels. vbar assignment where logical. When TRUE, the default, the name of the function and the assignment arrow are printed to the ﬁle. speciﬁes placement of ﬂoats if a table environment is used. Default is "!tbp". To allow tables to appear in the middle of a page of text you might specify where="!htbp" to latex.default. size of table text if a size change is needed (default is no change). For example you might specify size="small" to use LaTeX font size "small". default is "center" to enclose the table in a center environment. Use center="centering" to instead use a LaTeX centering directive, or center="none" to use no centering. This option was implemented by Markus Jäntti markus.jantti@iki.ﬁ of Abo Akademi University. set to TRUE to enclose the table in a landscape environment. When ctable is TRUE, will use the rotate argument to ctable. The default uses the S Example environment for latex.function, assuming you have installed S.sty in a location that the system latex command automatically accesses. Set type="verbatim" to instead use the LaTeX verbatim environment. other arguments are accepted and ignored except that latex passes arguments to format.df (e.g., col.just and other formatting options like dec, rdec, and cdec). For latexVerbatim these arguments are passed to the print function. Ignored for latexTranslate. specify additional input and translated strings over the usual defaults If pb=TRUE, latexTranslate also translates [()] to math mode using left, right. set to TRUE to have latexTranslate put names for greek letters in math mode and add backslashes horizontal space, e.g., extra left margin for verbatim text. Default is none. Use e.g. hspace="10ex" to add 10 extra spaces to the left of the text. size center landscape type ... inn, out pb greek hspace 142 length width height latex for S-Plus only; is the length of the output page for printing and capturing verbatim text are the options( ) to have in effect only for when print is executed. Defaults are current options. For dvi these specify the paper width and height in inches if nomargins=TRUE, with defaults of 5.5 and 7, respectively. set to TRUE to have dvi print, to the S-Plus session, the LaTeX .log ﬁle. set to FALSE to not use multicolumn in header of table set to FALSE to use default LaTeX margins when making the .dvi ﬁle prlog multicol nomargins Details If running under Windows and using MikTeX, latex and yap must be in your system path, and yap is used to browse .dvi ﬁles created by latex. You should install the geometry and ctable styles in MikTeX to make optimum use of latex(). System options can be used to specify external commands to be used. Defaults are given by options(xdvicmd=’xdvi’) or options(xdvicmd=’yap’), options(dvipscmd=’dvips’), options(latexcmd=’latex’). For MacOS specify options(xdvicmd=’MacdviX’). If running S-Plus and your directory for temporary ﬁles is not /tmp (Unix/Linux) or \windows\temp (Windows), add your own tempdir function such as tempdir = 1, or to show all values less than the nout quantile or greater than the 1-nout quantile if 0 0 nout datadensity scat1d.opts font pch cex col ... stats xlim xlab qomit cex.labels cex.points grid see panel.bwplot arguments passed to points undocumented arguments to bpplt 160 Author(s) Frank Harrell Department of Biostatistics Vanderbilt University School of Medicine f.harrell@vanderbilt.edu References panel.bpplot Esty, W. W. and Banﬁeld, J. D. (1992) "The Box-Percentile Plot," Technical Report (May 15, 1992), Department of Mathematical Sciences, Montana State University. See Also bpplot, panel.bwplot, scat1d, quantile, Ecdf Examples
set.seed(13) x 125 observations # small dot for means, show only .05,.125,.25,.375,.625,.75,.875,.95 quantiles bwplot(g ~ x, panel=panel.bpplot, cex=.3) # suppress means and reference lines for lower and upper quartiles bwplot(g ~ x, panel=panel.bpplot, probs=c(.025,.1,.25), means=FALSE, qref=FALSE) # continuous plot up until quartiles ("Tootsie Roll plot") bwplot(g ~ x, panel=panel.bpplot, probs=seq(.01,.25,by=.01)) # start at quartiles then make it continuous ("coffin plot") bwplot(g ~ x, panel=panel.bpplot, probs=seq(.25,.49,by=.01)) # same as previous but add a spike to give 0.95 interval bwplot(g ~ x, panel=panel.bpplot, probs=c(.025,seq(.25,.49,by=.01))) # decile plot with reference lines at outer quintiles and median bwplot(g ~ x, panel=panel.bpplot, probs=c(.1,.2,.3,.4), qref=c(.5,.2,.8)) # default plot with tick marks showing all observations outside the outer # box (.05 and .95 quantiles), with very small ticks partition 161 bwplot(g ~ x, panel=panel.bpplot, nout=.05, scat1d.opts=list(frac=.01)) # show 5 smallest and 5 largest observations bwplot(g ~ x, panel=panel.bpplot, nout=5) # Use a scat1d option (preserve=TRUE) to ensure that the right peak extends # to the same position as the extreme scat1d bwplot(~x , panel=panel.bpplot, probs=seq(.00,.5,by=.001), datadensity=TRUE, scat1d.opt=list(preserve=TRUE)) # Draw a prototype showing how to interpret the plots bpplt() # make a local copy of bwplot that always uses panel.bpplot (S-Plus only) # bwplot$panel 2) 3 else 0, bass=0, trim, fun, group, prefix, xlim, ylim, label.curves=TRUE, datadensity=FALSE, lines.=TRUE, subset=TRUE, grid=FALSE, ...) #To use panel function: #xyplot(formula=y ~ x conditioningvars, groups, # panel=panel.plsmo, type='b', # label.curves=TRUE, # lwd = superpose.line$lwd, # lty = superpose.line$lty, # pch = superpose.symbol$pch, # cex = superpose.symbol$cex, # font = superpose.symbol$font, # col = NULL, ...) Arguments x y method xlab vector of x-values, NAs allowed vector of y-values, NAs allowed "lowess" (the default), "supsmu", or "raw" to not smooth at all x-axis label iff add=F. Defaults of label(x) or argument name. plsmo ylab add lty col lwd y-axis label, like xlab. Set to T to call lines instead of plot. Assumes axes already labeled. line type, default=1,2,3,. . . , corresponding to group 165 color for each curve, corresponding to group. Default is current par("col"). vector of line widths for the curves, corresponding to group. Default is current par("lwd"). lwd can also be speciﬁed as an element of label.curves if label.curves is a list. iter parameter if method="lowess", default=0 if y is binary, and 3 otherwise. bass parameter if method="bass", default=0. only plots smoothed estimates between trim and 1-trim quantiles of x. Default is to use 10th smallest to 10th largest x in the group if the number of observations in the group exceeds 200 (0 otherwise). Specify trim=0 to plot over entire range. after computing the smoothed estimates, if fun is given the y-values are transformed by fun() a variable, either a factor vector or one that will be converted to factor by plsmo, that is used to stratify the data so that separate smooths may be computed a character string to appear in group of group labels. The presence of prefix ensures that labcurve will be called even when add=TRUE. a vector of 2 x-axis limits. Default is observed range. a vector of 2 y-axis limits. Default is observed range. iter bass trim fun group prefix xlim ylim label.curves set to FALSE to prevent labcurve from being called to label multiple curves corresponding to groups. Set to a list to pass options to labcurve. lty and col are passed to labcurve automatically. datadensity lines. subset grid ... type pch cex font vectors of graphical parameters corresponding to the groups (scalars if group is absent). By default, the parameters set up by trellis will be used. set to TRUE to draw tick marks on each curve, using x-coordinates of the raw data x values. This is done using scat1d. set to FALSE to suppress smoothed curves from being drawn. This can make sense if datadensity=TRUE. a logical or integer vector specifying a subset to use for processing, with respect too all variables being analyzed set to TRUE if the R grid package drew the current plot optional arguments that are passed to scat1d, or optional parameters to pass to plsmo from panel.plsmo. See optional arguments for plsmo above. set to p to have panel.plsmo plot points (and not call plsmo), l to call plsmo and not plot points, or use the default b to plot both. Value plsmo returns a list of curves (x and y coordinates) that was passed to labcurve 166 Side Effects plots, and panel.plsmo creates the Key function in the session frame. See Also plsmo lowess, supsmu, label, quantile, labcurve, scat1d, xyplot, panel.superpose, panel.xyplot Examples
set.seed(1) x 5, the outer quantiles are .025 and .975. The knots are equally spaced between these on the quantile scale. For fewer than 100 non-missing values of x, the outer knots are the 5th smallest and largest x. number of knots. Default is 5. The minimum value is 3. set to TRUE to add x as the ﬁrst column of the returned matrix return the estimated knot locations but not the expanded matrix "ordinary" to ﬁt the function, "integral" to ﬁt its anti-derivative. 0 to use the terms as originally given by Devlin and Weeks (1986), 1 to normalize non-linear terms by the cube of the spacing between the last two knots, 2 to normalize by the square of the spacing between the ﬁrst and last knots (the default). norm=2 has the advantage of making all nonlinear terms be on the x-scale. If given, any NAs in x will be replaced with the value rpm after estimating any knot locations. nk inclx knots.only type norm rpm Value If knots.only=TRUE, returns a vector of knot locations. Otherwise returns a matrix with x (if inclx=TRUE) followed by nk-2 nonlinear terms. The matrix has an attribute knots which is the vector of knots used. rcspline.plot References 185 Devlin TF and Weeks BJ (1986): Spline functions for logistic regression modeling. Proc 11th Annual SAS Users Group Intnl Conf, p. 646–651. Cary NC: SAS Institute, Inc. See Also ns, rcspline.restate, rcs Examples
x 10) # rowpct 0 rather than missing(), so it’s easier to deal with non-applicable parameters. But when grid is in effect, the preferred function to use is rlegendg, which calls the lattice draw.key function. Usage rlegend(x, y, legend, fill, col = "black", lty = NULL, lwd = NULL, pch = NULL, angl rlegendg(x, y, legend, col=pr$col[1], lty=NULL, lwd=NULL, pch=NULL, cex=pr$cex[1], other=NULL) 194 Arguments x y legend fill col lty lwd pch angle density bty bg pt.bg cex xjust yjust x.intersp y.intersp adj text.width merge trace ncol horiz plot grid ... other rlegend see legend set to FALSE to suppress drawing the legend. This is used the compute the size needed for when the legend is drawn with a later call to rlegend. set to TRUE if the grid package is in effect see legend a list containing other arguments to pass to draw.key. See the help ﬁle for xyplot. Value a list with elements rect and text. rect has elements w, h, left, top with size/position information. Author(s) Frank Harrell and R-Core See Also legend,draw.key rm.boot 195 rm.boot Bootstrap Repeated Measurements Model Description For a dataset containing a time variable, a scalar response variable, and an optional subject identiﬁcation variable, obtains least squares estimates of the coefﬁcients of a restricted cubic spline function or a linear regression in time after adjusting for subject effects through the use of subject dummy variables. Then the ﬁt is bootstrapped B times, either by treating time and subject ID as ﬁxed (i.e., conditioning the analysis on them) or as random variables. For the former, the residuals from the original model ﬁt are used as the basis of the bootstrap distribution. For the latter, samples are taken jointly from the time, subject ID, and response vectors to obtain unconditional distributions. If a subject id variable is given, the bootstrap sampling will be based on samples with replacement from subjects rather than from individual data points. In other words, either none or all of a given subject’s data will appear in a bootstrap sample. This cluster sampling takes into account any correlation structure that might exist within subjects, so that conﬁdence limits are corrected for within-subject correlation. Assuming that ordinary least squares estimates, which ignore the correlation structure, are consistent (which is almost always true) and efﬁcient (which would not be true for certain correlation structures or for datasets in which the number of observation times vary greatly from subject to subject), the resulting analysis will be a robust, efﬁcient repeated measures analysis for the one-sample problem. Predicted values of the ﬁtted models are evaluated by default at a grid of 100 equally spaced time points ranging from the minimum to maximum observed time points. Predictions are for the average subject effect. Pointwise conﬁdence intervals are optionally computed separately for each of the points on the time grid. However, simultaneous conﬁdence regions that control the level of conﬁdence for the entire regression curve lying within a band are often more appropriate, as they allow the analyst to draw conclusions about nuances in the mean time response proﬁle that were not stated apriori. The method of Tibshirani (1997) is used to easily obtain simultaneous conﬁdence sets for the set of coefﬁcients of the spline or linear regression function as well as the average intercept parameter (over subjects). Here one computes the objective criterion (here both the -2 log likelihood evaluated at the bootstrap estimate of beta but with respect to the original design matrix and response vector, and the sum of squared errors in predicting the original response vector) for the original ﬁt as well as for all of the bootstrap ﬁts. The conﬁdence set of the regression coefﬁcients is the set of all coefﬁcients that are associated with objective function values that are less than or equal to say the 0.95 quantile of the vector of B + 1 objective function values. For the coefﬁcients satisfying this condition, predicted curves are computed at the time grid, and minima and maxima of these curves are computed separately at each time point to derive the ﬁnal simultaneous conﬁdence band. By default, the log likelihoods that are computed for obtaining the simultaneous conﬁdence band assume independence within subject. This will cause problems unless such log likelihoods have very high rank correlation with the log likelihood allowing for dependence. To allow for correlation or to estimate the correlation function, see the cor.pattern argument below. 196 Usage rm.boot rm.boot(time, y, id=seq(along=time), subset, plot.individual=FALSE, bootstrap.type=c('x fixed','x random'), nk=6, knots, B=500, smoother=supsmu, xlab, xlim, ylim=range(y), times=seq(min(time),max(time),length=100), absorb.subject.effects=FALSE, rho=0, cor.pattern=c('independent','estimate'), ncor=10000, ...) ## S3 method for class 'rm.boot': plot(x, obj2, conf.int=.95, xlab=x$xlab, ylab=x$ylab, xlim, ylim=x$ylim, individual.boot=FALSE, pointwise.band=FALSE, curves.in.simultaneous.band=FALSE, col.pointwise.band=2, objective=c('-2 log L','sse','dep -2 log L'), add=FALSE, ncurves, multi=FALSE, multi.method=c('color','density'), multi.conf =c(.05,.1,.2,.3,.4,.5,.6,.7,.8,.9,.95,.99), multi.density=c( -1,90,80,70,60,50,40,30,20,10, 7, 4), multi.col =c( 1, 8,20, 5, 2, 7,15,13,10,11, 9, 14), subtitles=TRUE, ...) Arguments time y x id numeric time vector continuous numeric response vector of length the same as time. Subjects having multiple measurements have the measurements strung out. an object returned from rm.boot subject ID variable. If omitted, it is assumed that each time-response pair is measured on a different subject. subset subset of observations to process if not all the data plot.individual set to TRUE to plot nonparametrically smoothed time-response curves for each subject bootstrap.type speciﬁes whether to treat the time and subject ID variables as ﬁxed or random nk number of knots in the restricted cubic spline function ﬁt. The number of knots may be 0 (denoting linear regression) or an integer greater than 2 in which k knots results in k-1 regression coefﬁcients excluding the intercept. The default is 6 knots. vector of knot locations. May be speciﬁed if nk is omitted. number of bootstrap repetitions. Default is 500. knots B rm.boot smoother xlab xlim ylim 197 a smoothing function that is used if plot.individual=TRUE. Default is supsmu. label for x-axis. Default is "units" attribute of the original time variable, or "Time" if no such attribute was deﬁned using the units function. speciﬁes x-axis plotting limits. Default is to use range of times speciﬁed to rm.boot. for rm.boot this is a vector of y-axis limits used if plot.individual=TRUE. It is also passed along for later use by plot.rm.boot. For plot.rm.boot, ylim can be speciﬁed, to override the value stored in the object stored by rm.boot. The default is the actual range of y in the input data. a sequence of times at which to evaluated ﬁtted values and conﬁdence limits. Default is 100 equally spaced points in the observed range of time. absorb.subject.effects If TRUE, adjusts the response vector y before re-sampling so that the subjectspeciﬁc effects in the initial model ﬁt are all zero. Then in re-sampling, subject effects are not used in the models. This will downplay one of the sources of variation. This option is used mainly for checking for consistency of results, as the re-sampling analyses are simpler when absort.subject.effects=TRUE. times rho The log-likelihood function that is used as the basis of simultaneous conﬁdence bands assumes normality with independence within subject. To check the robustness of this assumption, if rho is not zero, the log-likelihood under multivariate normality within subject, with constant correlation rho between any two time points, is also computed. If the two log-likelihoods have the same ranks across re-samples, alllowing the correlation structure does not matter. The agreement in ranks is quantiﬁed using the Spearman rank correlation coefﬁcient. The plot method allows the non-zero intra-subject correlation log-likelihood to be used in deriving the simultaneous conﬁdence band. Note that this approach does assume homoscedasticity. More generally than using an equal-correlation structure, you can specify a function of two time vectors that generates as many correlations as the length of these vectors. For example, cor.pattern=function(time1,time2).2^(abs(time1time2)/10) would specify a dampening serial correlation pattern. cor.pattern can also be a list containing vectors x (a vector of absolute time differences) and y (a corresponding vector of correlations). To estimate the correlation function as a function of absolute time differences within subjects, specify cor.pattern="estimate". The products of all possible pairs of residuals (or at least up to ncor of them) within subjects will be related to the absolute time difference. The correlation function is estimated by computing the sample mean of the products of standardized residuals, stratiﬁed by absolute time difference. The correlation for a zero time difference is set to 1 regardless of the lowess estimate. NOTE: This approach fails in the presence of large subject effects; correcting for such effects removes too much of the correlation structure in the residuals. the maximum number of pairs of time values used in estimating the correlation function if cor.pattern="estimate" other arguments to pass to smoother if plot.individual=TRUE cor.pattern ncor ... 198 obj2 rm.boot a second object created by rm.boot that can also be passed to plot.rm.boot. This is used for two-sample problems for which the time proﬁles are allowed to differ between the two groups. The bootstrapped predicted y values for the second ﬁt are subtracted from the ﬁtted values for the ﬁrst ﬁt so that the predicted mean response for group 1 minus the predicted mean response for group 2 is what is plotted. The conﬁdence bands that are plotted are also for this difference. For the simultaneous conﬁdence band, the objective criterion is taken to be the sum of the objective criteria (-2 log L or sum of squared errors) for the separate ﬁts for the two groups. The times vectors must have been identical for both calls to rm.boot, although NAs can be inserted by the user of one or both of the time vectors in the rm.boot objects so as to suppress certain sections of the difference curve from being plotted. the conﬁdence level to use in constructing simultaneous, and optionally pointwise, bands. Default is 0.95. conf.int ylab label for y-axis. Default is the "label" attribute of the original y variable, or "y" if no label was assigned to y (using the label function, for example). individual.boot set to TRUE to plot the ﬁrst 100 bootstrap regression ﬁts pointwise.band set to TRUE to draw a pointwise conﬁdence band in addition to the simultaneous band curves.in.simultaneous.band set to TRUE to draw all bootstrap regression ﬁts that had a sum of squared errors (obtained by predicting the original y vector from the original time vector and id vector) that was less that or equal to the conf.int quantile of all bootstrapped models (plus the original model). This will show how the point by point max and min were computed to form the simultaneous conﬁdence band. col.pointwise.band color for the pointwise conﬁdence band. Default is 2, which defaults to red for default Windows S-PLUS setups. objective the default is to use the -2 log of the Gaussian likelihood for computing the simultaneous conﬁdence region. If neither cor.pattern nor rho was speciﬁed to rm.boot, the independent homoscedastic Gaussian likelihood is used. Otherwise the dependent homoscedastic likelihood is used according to the speciﬁed or estimated correlation pattern. Specify objective="sse" to instead use the sum of squared errors. set to TRUE to add curves to an existing plot. If you do this, titles and subtitles are omitted. when using individual.boot=TRUE or curves.in.simultaneous.band=TRUE, you can plot a random sample of ncurves of the ﬁtted curves instead of plotting up to B of them. set to TRUE to draw multiple simultaneous conﬁdence bands shaded with different colors. Conﬁdence levels vary over the values in the multi.conf vector. add ncurves multi multi.method speciﬁes the method of shading when multi=TRUE. Default is to use colors, with the default colors chosen so that when the graph is printed under S-Plus for Windows 4.0 to an HP LaserJet printer, the conﬁdence regions are naturally rm.boot 199 ordered by darkness of gray-scale. Regions closer to the point estimates (i.e., the center) are darker. Specify multi.method="density" to instead use densities of lines drawn per inch in the conﬁdence regions, with all regions drawn with the default color. The polygon function is used to shade the regions. vector of conﬁdence levels, in ascending order. Default is to use 12 conﬁdence levels ranging from 0.05 to 0.99. multi.density vector of densities in lines per inch corresponding to multi.conf. As is the convention in the polygon function, a density of -1 indicates a solid region. multi.conf multi.col subtitles Details Observations having missing time or y are excluded from the analysis. As most repeated measurement studies consider the times as design points, the ﬁxed covariable case is the default. Bootstrapping the residuals from the initial ﬁt assumes that the model is correctly speciﬁed. Even if the covariables are ﬁxed, doing an unconditional bootstrap is still appropriate, and for large sample sizes unconditional conﬁdence intervals are only slightly wider than conditional ones. For moderate to small sample sizes, the "x random" method can be fairly conservative. If not all subjects have the same number of observations (after deleting observations containing missing values) and if bootstrap.type="x fixed", bootstrapped residual vectors may have a length m that is different from the number of original observations n. If m > n for a bootstrap repetition, the ﬁrst n elements of the randomly drawn residuals are used. If m =0) prn(c(i=i,null.in.region=null.in.region)) } # Simulation Results: 905/1000 simultaneous confidence bands # fully contained the horizontal line at zero ## End(Not run) samplesize.bin Sample Size for 2-sample Binomial Description Computes sample size(s) for 2-sample binomial problem given vector or scalar probabilities in the two groups. samplesize.bin Usage samplesize.bin(alpha, beta, pit, pic, rho=0.5) Arguments alpha beta pit pic rho Value TOTAL sample size(s) AUTHOR Rick Chappell Dept. of Statistics and Human Oncology University of Wisconsin at Madison chappell@stat.wisc.edu Examples
alpha 1000). By default, histSpike bins the continuous x variable into 100 equal-width bins and then computes the frequency counts within bins (if n does not exceed 10, no binning is done). If add=FALSE (the default), the function displays either proportions or frequencies as in a vertical histogram. Instead of bars, spikes are used to depict the frequencies. If add=FALSE, the function assumes you are adding small density displays that are intended to take up a small amount of space in the margins of the overall plot. The frac argument is used as with scat1d to determine the relative length of the whole plot that is used to represent the maximum frequency. No jittering is done by histSpike. 208 scat1d histSpike can also graph a kernel density estimate for x, or add a small density curve to any of 4 sides of an existing plot. When y or curve is speciﬁed, the density or spikes are drawn with respect to the curve rather than the x-axis. Usage scat1d(x, side=3, frac=0.02, jitfrac=0.008, tfrac, eps=ifelse(preserve,0,.001), lwd=0.1, col=par("col"), y=NULL, curve=NULL, bottom.align=FALSE, preserve=FALSE, fill=1/3, limit=TRUE, nhistSpike=2000, nint=100, type=c('proportion','count','density'), grid=FALSE, ...) jitter2(x, ...) ## Default S3 method: jitter2(x, fill=1/3, limit=TRUE, eps=0, presorted=FALSE, ...) ## S3 method for class 'data.frame': jitter2(x, ...) datadensity(object, ...) ## S3 method for class 'data.frame': datadensity(object, group, which=c("all","continuous","categorical"), method.cat=c("bar","freq"), col.group=1:10, n.unique=10, show.na=TRUE, nint=1, naxes, q, bottom.align=nint>1, cex.axis=sc(.5,.3), cex.var=sc(.8,.3), lmgp=NULL, tck=sc(-.009,-.002), ranges=NULL, labels=NULL, ...) # sc(a,b) means default to a if number of axes =50, use # linear interpolation within 3-50 histSpike(x, side=1, nint=100, frac=.05, minf=NULL, mult.width=1, type=c('proportion','count','density'), xlim=range(x), ylim=c(0,max(f)), xlab=deparse(substitute(x)), ylab=switch(type,proportion='Proportion', count ='Frequency', density ='Density'), y=NULL, curve=NULL, add=FALSE, bottom.align=type=='density', col=par('col'), lwd=par('lwd'), grid=FALSE, ...) scat1d Arguments x object side frac a vector of numeric data, or a data frame (for jitter2) 209 a data frame or list (even with unequal number of observations per variable, as long as group is not speciﬁed) axis side to use (1=bottom (default for histSpike), 2=left, 3=top (default for scat1d), 4=right) fraction of smaller of vertical and horizontal axes for tick mark lengths. Can be negative to move tick marks outside of plot. For histSpike, this is the relative length to be used for the largest frequency. When scat1d calls histSpike, it multiplies its frac argument by 2.5. fraction of axis for jittering. If 1. In other words, if you are only labeling the ﬁrst and last axis tick mark, the scat1d tick marks are centered on the variable’s axis. preserve fill set to TRUE to invoke jitter2 maximum fraction of the axis ﬁlled by jittered values. If d are duplicated values between a lower value l and upper value u, then d will be spread within +/fill*min(u-d,d-l)/2. speciﬁes a limit for maximum shift in jittered values. Duplicate values will be spread within +/- fill*min(limit,min(u-d,d-l)/2). The default TRUE restricts jittering to the smallest min(u-d,d-l)/2 observed and results in equal amount of jittering for all d. Setting to FALSE allows for locally different amount of jittering, using maximum space available. limit 210 nhistSpike scat1d If the number of observations exceeds or equals nhistSpike, scat1d will automatically call histSpike to draw the data density, to prevent the graphics ﬁle from being too large. used by or passed to histSpike. Set to "count" to display frequency counts rather than relative frequencies, or "density" to display a kernel density estimate computed using the density function. set to TRUE if the R grid package is in effect for the current plot number of intervals to divide each continuous variable’s axis for datadensity. For histSpike, is the number of equal-width intervals for which to bin x, and if instead nint is a character string (e.g., nint="all"), the frequency tabulation is done with no binning. In other words, frequencies for all unique values of x are derived and plotted. optional arguments passed to scat1d from datadensity or to histSpike from scat1d set to TRUE to prevent from sorting for determining the order l70 2:previous.disease score.binary(age>70, previous.disease, retfactor=FALSE) #Same as above but return factor variable with levels "none" "age>70" # "previous.disease" score.binary(age>70, previous.disease) #Additive scale with weights 1:age>70 2:previous.disease score.binary(age>70, previous.disease, fun=sum) #Additive scale, equal weights score.binary(age>70, previous.disease, fun=sum, points=c(1,1)) #Same as saying points=1 #Union of variables, to create a new binary variable score.binary(age>70, previous.disease, fun=any) sedit Character String Editing and Miscellaneous Character Handling Functions sedit Description 215 This suite of functions was written to implement many of the features of the UNIX sed program entirely within S-PLUS (function sedit). The substring.location function returns the ﬁrst and last position numbers that a sub-string occupies in a larger string. The substring2 store in .Data stores(x, y, z) #store x,y,z in .Data under names x,y,z storeTemp(x) #put x as name 'x' in frame 0 #for R, attach .GlobalTemp and store it there storeTemp(x,'X') #same as previous but under the name X ## End(Not run) 232 string.bounding.box string.bounding.box Determine Diamentions of Strings Description This determins the number of rows and maximum number of columns of each string in a vector. Usage string.bounding.box(string) Arguments string vector of strings Value rows columns vector containing the number of character rows in each string. vector containing the maximum number of character columns in each string. Note compatable with Splus string.bounding.box Author(s) Charles Dupont See Also nchar, stringDims Examples
a 1, llist(size=cut2(sz, g=4), bone), mean, stat.name='Proportion') dotplot(Proportion ~ size bone, data=s7) ## End(Not run) set.seed(1) temperature 0), prn = TRUE, abbreviate.dimnames = FALSE, prefix.width = max(nchar(v)), min.colwidth, formatArgs = NULL, ...) ## S3 method for class 'summary.formula.cross': latex(object, title = first.word(deparse(substitute(object))), twoway = nvar == 2, prnmiss = TRUE, prn = TRUE, caption=attr(object,"heading"), vnames=c('labels','names'), rowlabel="", ...) stratify(..., na.group=FALSE, shortlabel=TRUE) ## S3 method for class 'summary.formula.cross': formula(x, ...) cumcategory(y) mChoice(..., label='', sort.levels=c('original','alphabetic'), add.none=TRUE, none.name='none', na.result=FALSE, drop=TRUE) ## S3 method for class 'mChoice': as.character(x, sep=",", ...) Arguments formula An S formula with additive effects. For method="response" or "cross", the dependent variable has the usual connotation. For method="reverse", the dependent variable is what is usually thought of as an independent variable, and it is one that is used to stratify all of the right hand side variables. For method="response" (only), the formula may contain one or more invocations of the stratify function whose arguments are deﬁned below. This summary.formula 243 causes the entire analysis to be stratiﬁed by cross-classiﬁcations of the combined list of stratiﬁcation factors. This stratiﬁcation will be reﬂected as major column groupings in the resulting table, or as more response columns for plotting. If formula has no dependent variable method="reverse" is the only legal value and so method defaults to "reverse" in this case. x y data subset na.action an object created by summary.formula or mChoice a numeric, character, category, or factor vector for cumcategory. Is converted to a categorical variable is needed. name or number of a data frame. Default is the current frame. a logical vector or integer vector of subscripts used to specify the subset of data to use in the analysis. The default is to use all observations in the data frame. function for handling missing data in the input data. The default is a function deﬁned here called na.retain, which keeps all observations for processing, with missing variables or not. function for summarizing data in each cell. Default is to take the mean of each column of the possibly multivariate response variable. You can specify fun="%" to compute percentages (100 times the mean of a series of logical or binary variables). User–speciﬁed functions can also return a matrix. For example, you might compute quartiles on a bivariate response. The default is "response", in which case the response variable may be multivariate and any number of statistics may be used to summarize them. Here the responses are summarized separately for each of any number of independent variables. Continuous independent variables (see the continuous parameter below) are automatically stratiﬁed into g (see below) quantile groups (if you want to control the discretization for selected variables, use the cut2 function on them). Otherwise, the data are subsetted by all levels of discrete right hand side variables. For multivariate responses, subjects are considered to be missing if any of the columns is missing. The method="reverse" option is typically used to make baseline characteristic tables, for example. The single left hand side variable must be categorical (e.g., treatment), and the right hand side variables are broken down one at a time by the "dependent" variable. Continuous variables are described by three quantiles (quartiles by default) along with outer quantiles (used only for scaling x-axes when plotting quartiles; all are used when plotting box-percentile plots), and categorical ones are described by counts and percentages. If there is no left hand side variable, summary assumes that there is only one group in the data, so that only one column of summaries will appear. If there is no dependent variable in formula, method defaults to "reverse" automatically. The method="cross" option allows for a multivariate dependent variable and for up to three independents. Continuous independent variables (those with at least continuous unique values) are automatically divided into g quantile groups. The independents are cross-classiﬁed, and marginal statistics may optionally be computed. The output of summary.formula in this case is a data frame containing the independent variable combinations (with levels of "All" corresponding to marginals) and the corresponding summary statistics in the matrix S. The output data frame is suitable for direct use in trellis. fun method 244 summary.formula The print and latex typesetting methods for this method allows for a special two-way format if there are two right hand variables. For method="reverse", setting overall=TRUE makes a new column with overall statistics for the whole sample. For method="cross", overall=TRUE (the default) results in all marginal statistics being computed. For trellis displays (usually multi-panel dot plots), these marginals just form other categories. For "response", the default is overall=TRUE, causing a ﬁnal row of global summary statistics to appear in tables and dot charts. If test=TRUE these marginal statistics are ignored in doing statistical tests. speciﬁes the threshold for when a variable is considered to be continuous (when there are at least continuous unique values). factor variables are always considered to be categorical no matter how many levels they have. TRUE (the default) to exclude NAs before passing data to fun to compute statistics, FALSE otherwise. na.rm=FALSE is useful if the response variable is a matrix and you do not wish to exclude a row of the matrix if any of the columns in that row are NA. na.rm also applies to summary statistic functions such as smean.cl.normal. For these na.rm defaults to TRUE unlike built-in functions. for method="response", set na.include=FALSE to exclude missing values from being counted as their own category when subsetting the response(s) by levels of a categorical variable. For method="reverse" set na.include=TRUE to keep missing values of categorical variables from being excluded from the table. number of quantile groups to use when variables are automatically categorized with method="response" or "cross" using cut2 if fewer than nmin observations exist in a category for "response" (over all strata combined), that category will be ignored. For "reverse", for categories of the response variable in which there are less than or equal to nmin nonmissing observations, the raw data are retained for later plotting in place of box plots. applies if method="reverse". Set to TRUE to compute test statistics using tests speciﬁed in conTest and catTest. a function of two arguments (grouping variable and a continuous variable) that returns a list with components P (the computed P-value), stat (the test statistic, either chi-square or F), df (degrees of freedom), testname (test name), statname (statistic name), an optional component latexstat (LaTeX representation of statname), an optional component plotmathstat (for R the plotmath representation of statname, as a character string), and an optional component note that contains a character string note about the test (e.g., "test not done because n collapse 3 quartiles #Summarize only bilirubin, but do it with two statistics: #the mean and the median. Make separate tables for the two randomized #groups and make plots for the active arm. g 1 ~ size.quartile + bone, method='cross') #In this case, quartiles are the default so could have said sz + bone options(digits=3) print(s7, twoway=FALSE) s7 # same as print(s7) w 1", ylab="Quartile of Tumor Size") #Can do this more quickly with summarize: # s7 1, llist(size=cut2(sz, g=4), bone), mean, # stat.name='Proportion') # dotplot(Proportion ~ size bone, data=s7) summary(age ~ stage, method='cross') summary(age ~ stage, fun=quantile, method='cross') summary(age ~ stage, fun=smean.sd, method='cross') summary(age ~ stage, fun=smedian.hilow, method='cross') summary(age ~ stage, fun=function(x) c(Mean=mean(x), Median=median(x)), method='cross') #The next statements print real two-way tables summary(cbind(age,ap) ~ stage + bone, fun=function(y) apply(y, 2, quantile, c(.25,.75)), method='cross') options(digits=2) summary(log(ap) ~ sz + bone, fun=function(y) c(Mean=mean(y), quantile(y)), method='cross') #Summarize an ordered categorical response by all of the needed #cumulative proportions summary(cumcategory(disease.severity) ~ age + sex) ## End(Not run) symbol.freq Graphic Representation of a Frequency Table Description This function can be used to represent contingency tables graphically. Frequency counts are represented as the heights of "thermometers" by default; you can also specify symbol=’circle’ to the function. There is an option to include marginal frequencies, which are plotted on a halved scale so as to not overwhelm the plot. If you do not ask for marginal frequencies to be plotted using symbol.freq 255 marginals=T, symbol.freq will ask you to click the mouse where a reference symbol is to be drawn to assist in reading the scale of the frequencies. label attributes, if present, are used for x- and y-axis labels. Otherwise, names of calling arguments are used. Usage symbol.freq(x, y, symbol = c("thermometer", "circle"), marginals = FALSE, orig.scale = FALSE, inches = 0.25, width = 0.15, subset, srtx = 0, ...) Arguments x y symbol marginals orig.scale inches width subset srtx ... Author(s) Frank Harrell See Also symbols Examples
## Not run: getHdata(titanic) attach(titanic) age.tertile = 100 (4 parameters). Set to TRUE to return a list containing imputed values on the original scale. If the transformation for a variable is non-monotonic, imputed values are not unique. transcan uses the approx function, which returns the highest value of the variable with the transformed score equalling the imputed score. imputed=TRUE also causes original-scale imputed values to be shown as tick marks on the top margin of each graph when show.na=TRUE (for the ﬁnal iteration only). For categorical predictors, these imputed values are jittered so that their frequencies can be visualized. When n.impute is used, each NA will have n.impute tick marks. number of multiple imputations. If omitted, single predicted expected value imputation is used. n.impute=5 is frequently recommended. default is to use the approximate Bayesian bootstrap (sample with replacement from sample with replacement of the vector of residuals). You can also specify boot.method="simple" to use the usual bootstrap one-stage sampling with replacement. Set to TRUE to add an attribute trantab to the returned matrix. This contains a vector of lists each with components x and y containing the unique values and corresponding transformed values for the columns of x. This is set up to be used easily with the approx function. You must specify trantab=TRUE if you want to later use the predict.transcan function with type="original". set to TRUE to cause transcan to return an object transformed containing the matrix of transformed variables This argument tells how to impute categorical variables on the original scale. The default is impcat="score" to impute the category whose canonical variate score is closest to the predicted score. Use impcat="tree" to impute categorical variables using the tree() function, using the values of all other transformed predictors. impcat="rpart" will use rpart. A better but somewhat slower approach is to use impcat="multinom" to ﬁt a multinomial logistic model to the categorical variable, at the last iteraction of the transcan algorithm. This uses the multinom function in the nnet library asis nk imputed n.impute boot.method trantab transformed impcat transcan 271 of the MASS package (which is assumed to have been installed by the user) to ﬁt a polytomous logistic model to the current working transformations of all the other variables (using conditional mean imputation for missing predictors). Multiple imputations are made by drawing multinomial values from the vector of predicted probabilities of category membership for the missing categorical values. mincut If imputed=TRUE, there are categorical variables, and impcat="tree", mincut speciﬁes the lowest node size that will be allowed to be split by tree. The default is 40. By default, imputed values are back-solved on the original scale using inverse linear interpolation on the ﬁtted tabulated transformed values. This will cause distorted distributions of imputed values (e.g., ﬂoor and ceiling effects) when the estimated transformation has a ﬂat or nearly ﬂat section. To instead use the invertTabulated function (see above) with the "sample" option, specify inverse="sample". the multiplyer of the range of transformed values, weighted by freq and by the distance measure, for determining the set of x values having y values within a tolerance of the value of aty in invertTabulated. For predict.transcan, inverse and tolInverse are obtained from options that were speciﬁed to transcan by default. Otherwise, if not speciﬁed by the user, these default to the defaults used to invertTabulated. For transcan, set to FALSE to suppress printing r-squares and shrinkage factors. For impute.transcan set to FALSE to suppress messages concerning the number of NAs imputed, or for fit.mult.impute set to FALSE to suppress printing variance inﬂation factors accounting for imputation, rate of missing information, and degrees of freedom. Set to FALSE to suppress plotting the ﬁnal transformations with distribution of scores for imputed values (if show.na=TRUE). Set to TRUE to plot transformations for intermediate iterations. inverse tolInverse pr pl allpl show.na Set to FALSE to suppress the distribution of scores assigned to missing values (as tick marks on the right margin of each graph). See also imputed. imputed.actual The default is "none" to suppress plotting of actual vs. imputed values for all variables having any NAs. Other choices are "datadensity" to use datadensity to make a single plot, "hist" to make a series of back-toback histograms, "qq" to make a series of q-q plots, or "ecdf" to make a series of empirical cdfs. For imputed.actual="datadensity" for example you get a rug plot of the non-missing values for the variable with beneath it a rug plot of the imputed values. When imputed.actual is not "none", imputed is automatically set to TRUE. iter.max maximum number of iterations to perform for transcan or predict. For predict, only one iteration is used if there are no NAs in the data or if imp.con was used. convergence criterion for transcan and predict. eps is the maximum change in transformed values from one iteration to the next. If for a given iteration all new transformations of variables differ by less than eps (with or eps 272 transcan without negating the transformation to allow for "ﬂipping") from the transformations in the previous iteration, one more iteration is done for transcan. During this last iteration, individual transformations are not updated but coefﬁcients of transformations are. This improves stability of coefﬁcients of canonical variates on the right-hand-side. eps is ignored when rhsImp="random". curtail for transcan, causes imputed values on the transformed scale to be truncated so that their ranges are within the ranges of non-imputed transformed values. For predict, curtail defaults to TRUE to truncate predicted transformed values to their ranges in the original ﬁt (xt). for transcan, set to TRUE to impute NAs on the original scales with constants (medians or most frequent category codes). Set to a vector of constants to instead always use these constants for imputation. These imputed values are ignored when ﬁtting the current working transformation for a single variable. default is FALSE to use ordinary least squares or canonical variate estimates. For the purposes of imputing NAs, you may want to set shrink=TRUE to avoid overﬁtting when developing a prediction equation to predict each variables from all the others (see details below). method for initializing scorings of categorical variables. Default is "mode" to use a dummy variable set to 1 if the value is the most frequent value (this is the default). Use "random" to use a random 0-1 variable. Set to "asis" to use the original integer codes as starting scores. number of residuals to store if n.impute is speciﬁed. If the dataset has fewer than nres observations, all residuals are saved. Otherwise a random sample of the residuals of length nres without replacement is saved. The default for nres is higher if boot.method="approximate bayesian". an integer or logical vector specifying the subset of observations to ﬁt These may be used if x is a formula. The default na.action is na.retain (deﬁned by transcan) which keeps all observations with any NAs. For impute.transcan, data is a data frame to use as the source of variables to be imputed, rather than using where.in. For fit.mult.impute, data is mandatory and is a data frame containing the data to be used in ﬁtting the model but before imputations are applied. Variables omitted from data are assumed to be available from frame 1 and do not need to be imputed. Set to TRUE to get additional information printed when impcat="tree", such as the predicted probabilities of category membership. imp.con shrink init.cat nres data subset na.action treeinfo rhsImp Set to "random" to use random draw imputation when a sometimes missing variable is moved to be a predictor of other sometimes missing variables. Default is rhsImp="mean", which uses conditional mean imputation on the transformed scale. Residuals used are residuals from the transformed scale. When "random" is used, transcan runs 5 iterations and ignores eps. details.impcat set to a character scalar that is the name of a category variable to include in the resulting transcan object an element details.impcat containing details of how the categorical variable was multiply imputed. transcan ... long var 273 arguments passed to scat1d or to the fitter function (for fit.mult.impute) for summary, set to TRUE to print all imputed values. For print, set to TRUE to print details of transformations/imputations. For impute, is a variable that was originally a column in x, for which imputated values are to be ﬁlled in. imputed=TRUE must have been used in transcan. Omit var to impute all variables, creating new variables in search position where. speciﬁes which of the multiple imputations to use for ﬁlling in NAs name of variable to impute, for impute(). Default is character string version of the second argument (var) in the call to impute. For invertTabulated, is the name of variable being transformed (used only for warning messages). location in search list to ﬁnd variables that need to be imputed, when all variables are to be imputed automatically by impute.transcan (i.e., when no input variable name is speciﬁed). Default is ﬁrst search position that contains the ﬁrst variable to be imputed. location in the search list for storing variables with missing values set to imputed values, for impute.transcan when all variables with missing values are being imputed automatically. Instead of specifying where.out you can specify an S frame number into which individual new imputed variables will be written. For example, frame.out=1 is useful for putting new variables into a temporary local frame when impute is called within another function (see fit.mult.impute). See assign for details about frames. For R, where.out and frame.out are ignored and results are stored in .GlobalEnv when list.out is not speciﬁed (it is recommended to use list.out=TRUE). If var is not speciﬁed, you can set list.out=TRUE to have impute.transcan return a list containing variables with needed values imputed. This list will contain a single imputation. set to FALSE to suppress certain warning messages a new data matrix for which to compute transformed variables. Categorical variables must use the same integer codes as were used in the call to transcan. If a formula was originally speciﬁed to transcan (instead of a data matrix), newdata is optional and if given must be a data frame; a model frame is generated automatically from the previous formula. The na.action is handled automatically, and the levels for factor variables must be the same and in the same order as were used in the original variables speciﬁed in the formula given to transcan. set to TRUE to save all ﬁt objects from the ﬁt for each imputation in fit.mult.impute. Then the object returned will have a component fits which is a list whose ith element is the ith ﬁt object. an expression containing S expressions for computing derived variables that are used in the model formula. This is useful when multiple imputations are done for component variables but the actual model uses combinations of these (e.g., ratios or other derivations). For a single derived variable you can speciﬁed for example derived=expression(ratio 0, transcan also returns a list residuals that can be used for future multiple imputation. impute returns a vector (the same length as var) of class "impute" with NAs imputed. predict returns a matrix with the same number of columns or variables as were in x. fit.mult.impute returns a ﬁt object that is a modiﬁcation of the ﬁt object created by ﬁtting the completed dataset for the ﬁnal imputation. The var matrix in the ﬁt object has the imputationcorrected variance-covariance matrix. coefficients is the average (over imputations) of the coefﬁcient vectors, variance.inflation.impute is a vector containing the ratios of the diagonals of the between-imputation variance matrix to the diagonals of the average apparent (withinimputation) variance matrix. missingInfo is Rubin’s "rate of missing information" and dfmi is Rubin’s degrees of freedom for a t-statistic for testing a single parameter. The last two objects are vectors corresponding to the diagonal of the variance matrix. Side Effects prints, plots, and impute.transcan creates new variables. Author(s) Frank Harrell Department of Biostatistics transcan Vanderbilt University f.harrell@vanderbilt.edu References 277 Kuhfeld, Warren F: The PRINQUAL Procedure. SAS/STAT User’s Guide, Fourth Edition, Volume 2, pp. 1265–1323, 1990. Van Houwelingen JC, Le Cessie S: Predictive value of statistical models. Statistics in Medicine 8:1303–1325, 1990. Copas JB: Regression, prediction and shrinkage. JRSS B 45:311–354, 1983. He X, Shen L: Linear regression after spline transformation. Biometrika 84:474–481, 1997. Little RJA, Rubin DB: Statistical Analysis with Missing Data. New York: Wiley, 1987. Rubin DJ, Schenker N: Multiple imputation in health-care databases: An overview and some applications. Stat in Med 10:585–598, 1991. Faris PD, Ghali WA, et al:Multiple imputation versus data enhancement for dealing with missing data in observational health care outcome analyses. J Clin Epidem 55:184–191, 2002. See Also aregImpute, impute, naclus, naplot, ace, avas, cancor, prcomp, rcspline.eval, lsfit, approx, datadensity, mice Examples
## Not run: x 4 are stored as S double precision numerics, which allow for the same precision as a SAS LENGTH 8 variable. Set force.single=T to store every numeric variable in single precision (7 digits of precision). This option is useful when the creator of the SAS dataset has failed to use a LENGTH statement. R does not have single precision, so no attempt is made to convert to single if running R. dates One of the character strings "sas", "yearfrac", "yearfrac2", "yymmdd". If a SAS variable has a date format (one of "DATE", "MMDDYY", "YYMMDD", "DDMMYY", "YYQ", "MONYY", "JULIAN"), it will be converted to the format speciﬁed by dates before being given to S. "sas" gives days from 1/1/1960 (from 1/1/1970 if using chron), "yearfrac" gives days from 1/1/1900 divided by 365.25, "yearfrac2" gives year plus fraction of current year, and "yymmdd" gives a 6 digit number YYMMDD (year%%100, month, day). Note that S will store these as numbers, not as character strings. If dates="sas" and a variable has one of the SAS date formats listed above, the variable will be given a class of "date" to work with Terry Therneau’s implemen- 308 sas.get tation of the "date" class in S. If the chron package or timeDate function is available, these are used instead. keep.log log.file macro logical ﬂag: if FALSE, delete the SAS log ﬁle upon completion. the name of the SAS log ﬁle. the name of an S object in the current search path that contains the text of the SAS macro called by S. The S object is a character vector that can be edited using for example sas.get.macro 
